# -*- coding: utf-8 -*-
"""App_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ur2VL4aA07wPpO0ChvJCWlsHHiZNWuSh
"""

import streamlit as st
import numpy as np
import pandas as pd
import re
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from collections import Counter

# -----------------------------
# Set device and hyperparameters
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Hyperparameters
seq_length = 20
embedding_dim = 256
hidden_dim = 512
dropout_rate = 0.3
batch_size = 64
epochs = 10  # Reduce epochs for faster execution
learning_rate = 0.0001

# -----------------------------
# Data Loading & Preprocessing
@st.cache_data
def load_and_preprocess_data(csv_file):
    df = pd.read_csv(csv_file)

    def clean_text(text):
        text = re.sub(r"[^a-zA-Z0-9\s'.,!?-]", "", text)
        text = re.sub(r"\s+", " ", text).strip()
        return text

    df["Poetry"] = df["Poetry"].apply(clean_text)
    full_text = " ".join(df["Poetry"].dropna().tolist())
    words = full_text.split()

    return words

words = load_and_preprocess_data("Roman-Urdu-Poetry.csv")

# -----------------------------
# Build vocabulary
word_counts = Counter(words)
vocab = list(word_counts.keys())
word2idx = {word: idx + 1 for idx, word in enumerate(vocab)}
idx2word = {idx: word for word, idx in word2idx.items()}
vocab_size = len(word2idx) + 1

# -----------------------------
# Create sequences
sequences = []
for i in range(len(words) - seq_length):
    seq = words[i:i + seq_length + 1]
    sequences.append(seq)

numerical_sequences = [[word2idx.get(word, 0) for word in seq] for seq in sequences]

padded_sequences = np.zeros((len(numerical_sequences), seq_length + 1), dtype=np.int32)
for i, seq in enumerate(numerical_sequences):
    padded_sequences[i, -len(seq):] = seq

X = padded_sequences[:, :-1]
y = padded_sequences[:, -1]

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = torch.tensor(X_train, dtype=torch.long).to(device)
y_train = torch.tensor(y_train, dtype=torch.long).to(device)
X_val = torch.tensor(X_val, dtype=torch.long).to(device)
y_val = torch.tensor(y_val, dtype=torch.long).to(device)

# -----------------------------
# Dataset & DataLoader
class PoetryDataset(Dataset):
    def __init__(self, X, y):
        self.X = X
        self.y = y
    def __len__(self):
        return len(self.X)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

train_dataset = PoetryDataset(X_train, y_train)
val_dataset = PoetryDataset(X_val, y_val)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)

# -----------------------------
# Define GRU Model
class PoetryGRU(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate=0.3):
        super(PoetryGRU, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, num_layers=2, dropout=dropout_rate)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout_rate)
    def forward(self, x):
        embedded = self.dropout(self.embedding(x))
        gru_out, _ = self.gru(embedded)
        output = self.fc(gru_out[:, -1, :])
        return output

model = PoetryGRU(vocab_size, embedding_dim, hidden_dim, vocab_size, dropout_rate).to(device)

# -----------------------------
# Train the Model (Instead of Loading It)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

st.write("Training model... ⏳")

for epoch in range(epochs):
    model.train()
    total_loss = 0
    for inputs, targets in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    st.write(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

st.write("Training complete! ✅")

# -----------------------------
# Poetry Generation
def generate_poetry(seed_text, next_words=20, temperature=1.0):
    model.eval()
    words_generated = seed_text.split()

    for _ in range(next_words):
        tokenized_input = [word2idx.get(word, 0) for word in words_generated[-seq_length:]]

        if len(tokenized_input) < seq_length:
            tokenized_input = [0] * (seq_length - len(tokenized_input)) + tokenized_input

        input_seq = torch.tensor(tokenized_input, dtype=torch.long).unsqueeze(0).to(device)

        with torch.no_grad():
            output = model(input_seq)

        output_dist = torch.nn.functional.softmax(output / temperature, dim=-1)
        predicted_idx = torch.multinomial(output_dist, num_samples=1).item()

        predicted_word = idx2word.get(predicted_idx, "")
        if not predicted_word:  # Skip empty words
            continue

        words_generated.append(predicted_word)

    return " ".join(words_generated)

# -----------------------------
# Streamlit UI
st.title("Roman Urdu Poetry Generator")
st.write("Generate beautiful Roman Urdu poetry using an AI-powered GRU model.")

seed_text_input = st.text_input("Enter a starting phrase:", "Mohabbat ek aisi cheez hai")
num_words = st.slider("Number of words to generate:", min_value=5, max_value=50, value=20)
temperature_input = st.slider("Temperature (creativity):", min_value=0.5, max_value=2.0, value=1.0, step=0.1)

if st.button("Generate Poetry"):
    generated_poetry = generate_poetry(seed_text_input, next_words=num_words, temperature=temperature_input)

    st.markdown("### Generated Poetry:")
    st.write(generated_poetry)

    st.download_button(
        "Download Poetry",
        generated_poetry,
        file_name="poetry.txt",
        mime="text/plain",
        key="download-btn"
    )

st.write("© 2025 Muhammad Bin Imran - All Rights Reserved")